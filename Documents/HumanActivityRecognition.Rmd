---
title: "Human Activity Recognition"
author: "Salvador Ruiz"
date: "25/07/2015"
output: html_document
---

## Executive summary 

As requirement for the course of *Practical Machine Learning* offered by the Johns Hopkins University on Coursera, this stydy 
has been performed.

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

Read more about the data set details [here] (http://groupware.les.inf.puc-rio.br/har).

The targets of this project are:

* Create a suitable model for prediction of activity.
* Evaluate how much accurate the model is using `out of sample` testing data.
* Using the prediction model, predict 20 different cases for submission to the course.

## Loading data

There are two functions created for extracting the data from source and loading as dataframes into `R`. 

```{r}

downloadDataset <- function(URL="", destFile="data.csv"){
# Function to download the csv file with th data
     if(!file.exists(destFile)){
          download.file(URL, destFile, method="curl")
     }else{
          message("Dataset already downloaded. Download is cancelled")
     }
}

readHARdataset  <- function(dataType = ""){
     URLfile  <- paste0("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-", dataType, ".csv")
     destFile  <- paste0("../RawData/", dataType, ".csv")
     downloadDataset(URLfile, destFile)
     read.csv(destFile, header = TRUE, na.strings = c("NA", "", "#DIV/0!"))
}

```


## Data partition

We have two datasets:

* `plm-training.csv` which is the raw data we receive for modeling and evaluate internally our model. From here we're extracting training and testing dataset.
* `plem-testing.csv` which is not really a testing dataset. It's data for submission results to external evaluation of our model. 

Let's begin with our raw data. The partition is being 70% for training and 30% for testing

```{r}
library(caret)

HRdata  <- readHARdataset("training")
set.seed(1234)
train  <- createDataPartition(HRdata$classe, p=0.7, list = FALSE)
training  <- HRdata[train, ]
testing  <- HRdata[-train, ]
```

## Preprocess data: feature selection

The number of fields for the data is `r ncol(training)`. One of them is the outcome `classe`, so there are `r ncol(training)-1` potential features. Too many for an efficient modeling.

It's needed to select only representative features. These are the criteria for removing features to model:

* Features are not related to any temporal parameter.
* We find a model to work with whatever user. User information shouldn't be a feature for prediction.
* Fields with more than 95% of values with NAs shouldn't affect to outcome.
* Fields with near zero variance.
* Fields which are correlated more than 0.90 to other fields


```{r}
getFeatures  <- function(data) {
# Return a logical array with all active features for modelling HAR
# Input shall be the training set
     
     featureNames  <- names(data)
    
     # Remove outcome
     features  <- !(featureNames %in% "classe")
     
     # Remove features with nonsense in the model
     # X: sequential number
     # user_name: We want to have a general model for all users..
     # raw_timestamp_part_1 : the model doesn't depends on time
     # raw_timestamp_part_2: the model doesn't depends on time
     # cvtd_timestamp: the model doesn't depends on time
     nonsenseFeatures  <- c("X", "user_name", "raw_timestamp_part_1",
                              "raw_timestamp_part_2", "cvtd_timestamp")
     features  <-  features & !(featureNames %in% nonsenseFeatures)
     message("5 non-sense features has been removed")
     
     # Remove features with more than 95% with NAs
     rowCounts  <- nrow(data)
     NAcounts  <- colSums(is.na(data))
     NAfeatures  <- NAcounts > 0.95*rowCounts
     message(paste(sum(NAfeatures),"columns has more than 95% with NAs"))
     features  <- features & !NAfeatures
     
     # Remove features with Zero varianze
     nz  <- nearZeroVar(data, saveMetrics = TRUE)$nzv
     message(paste(sum(nz),"columns are near to zero value"))
     features  <- features & !nz
     
    
     # Remove correlated features 
     data  <- data[,features]
     newFeatureNames  <- names(data)
     correlatedFeatures  <-  findCorrelation(cor(data), cutoff = 0.90)
     message(length(correlatedFeatures), " features have high correlation to others")
     features  <- features & !(featureNames %in% newFeatureNames[correlatedFeatures])
     
     features
}
```

```{r}
features  <- getFeatures(training)
trFeatures  <- training[, features]
trOutcome  <- training$classe
```

After the removal of the secondary fields we get `r ncol(trFeatures)` features for modelling. We've reduced to a third part the number of variables to model so this preprocessing will increase a lot performance on modelling.

## Modeling

The `random forest` algorithm has been chosen for modelling the prediction. The main reason is that this is one which has the best accuracy for classification problems. 

## Model fitting

For avoiding overfitting, one of the main risks in `random forest` algorithm, we've defined a cross validation with 4 k-folders.

```{r, cache=TRUE}

rfControl  <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
model <- train(x = trFeatures, y = trOutcome, method = "rf", prox = TRUE, trControl = rfControl)
model

```


## Model evaluation

Finally we insert testing data into model for prediction.

```{r}
     tsFeatures <- testing[,features]
     tsReference  <- testing$classe
     tsPrediction  <- predict(model, newdata = tsFeatures)
     cm <- confusionMatrix(tsPrediction, tsReference)
     cm
``` 

We get after evaluating testing data an accuracy of `r paste(round(cm$overall[1]*100,2), "%")`, which is a really good result.

## Submission

Once we have a proper model, we submit the evaluation data for the submission on Coursera site.

```{r}
# Loading testing data
submission <- readHARdataset("testing")

# Predicting answers
answers  <-  predict(model, submission[,features])

#Creating files for submission
pml_write_files = function(x){
     n = length(x)
     for(i in 1:n){
          filename = paste0("../Submission/problem_id_",i,".txt")
          write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
     }
}
 
pml_write_files(answers)     

```

All predictions have been evaluated as correct by Coursera web site.

## Conclusions

We've get a good accuracy ( `r paste(round(cm$overall[1]*100,2), "%")`) with the following actions:

* Use the 70% of data for modeling
* Reduce the number of features to `r ncol(trFeatures)` from `r ncol(training) - 1` 
* Use *random forest* as algorithm with cross validation with 4 k-folders.

### References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
